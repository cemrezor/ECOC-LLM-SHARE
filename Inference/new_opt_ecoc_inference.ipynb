{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, OPTPreTrainedModel, OPTModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import transformers\n",
    "from typing import Optional, Tuple, Union, List\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(nn.Module):\n",
    "    def __init__(self, ignore_value=-1.0):\n",
    "        super(MaskedLoss, self).__init__()\n",
    "        self.ignore_value = ignore_value\n",
    "        self.cross_loss = nn.BCELoss(reduction='none')  # Compute element-wise loss\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Create a mask to ignore entire rows filled with ignore_value\n",
    "        mask = ~(target == self.ignore_value)\n",
    "        loss = self.cross_loss(input, target)\n",
    "        nan_mask = torch.isnan(loss)\n",
    "        nan_indices = torch.nonzero(nan_mask, as_tuple=True)\n",
    "        loss = loss[mask]\n",
    "        if loss.numel() > 0:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=input.device)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_dictionary(file_path):\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        vocab_dict = json.load(json_file)\n",
    "    return vocab_dict\n",
    "\n",
    "token_binary_map = load_vocab_dictionary('/home/ec2-user/llms/ECOC/Training_scripts/vocab_dict_opt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTForCausalLM(OPTPreTrainedModel):\n",
    "    # _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = OPTModel(config)\n",
    "        # self.bit_size = math.ceil(math.log2(config.vocab_size))\n",
    "        self.bit_size = 50\n",
    "        self.linear_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config.word_embed_proj_dim, 128, bias=False),  # First linear layer\n",
    "                nn.ReLU(),  # Activation function\n",
    "                nn.Linear(128, 1, bias=False),  # Second linear layer, outputting a single logit\n",
    "                nn.Sigmoid()  # Sigmoid activation to convert logits to probabilities (0 or 1)\n",
    "            )\n",
    "            for _ in range(self.bit_size)\n",
    "        ])\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.decoder.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.decoder.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.linear_heads\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.linear_heads = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model.decoder = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.decoder\n",
    "    \n",
    "    def tie_weights(self):\n",
    "        # Override this method to prevent weight tying\n",
    "        pass\n",
    "\n",
    "    def find_closest_tensor(self, given_tensor, tensor_of_tensors):\n",
    "        assert given_tensor.shape[-1] == tensor_of_tensors.shape[-1], \"Shape mismatch between given tensor and tensor of tensors\"\n",
    "        distances = torch.mean((tensor_of_tensors - given_tensor) ** 2, dim=tuple(range(1, tensor_of_tensors.dim())))\n",
    "        min_index = torch.argmin(distances)\n",
    "        min_dist, max_dist = distances.min(), distances.max()\n",
    "        logits = 1 - (distances - min_dist) / (max_dist - min_dist + 1e-8)\n",
    "        closest_tensor = tensor_of_tensors[min_index]\n",
    "        min_distance = distances[min_index].item()\n",
    "        return closest_tensor, min_distance, logits\n",
    "\n",
    "    def int_to_bin_tensor(self, val):\n",
    "        if val==-100:\n",
    "            length = self.bit_size\n",
    "            bin_str = format(2, '0' + str(length) + 'b')\n",
    "            bin_tensor = torch.tensor([int(bit) for bit in bin_str])\n",
    "        else:\n",
    "            length = self.bit_size\n",
    "            bin_str = format(val, '0' + str(length) + 'b')\n",
    "            bin_tensor = torch.tensor([int(bit) for bit in bin_str])\n",
    "        return bin_tensor\n",
    "\n",
    "    # @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        logits = torch.stack([head(hidden_states) for head in self.linear_heads])\n",
    "        logits = logits.squeeze(-1)\n",
    "        logits = logits.permute(1, 2, 0) \n",
    "        loss = torch.tensor(0.0).to(logits.device)\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            binary_tensors = []\n",
    "            for i in range(shift_labels.shape[0]):\n",
    "                binary_tensors_row = []\n",
    "                for j in range(shift_labels.shape[1]):\n",
    "                    val = shift_labels[i, j].item()\n",
    "                    # print(val)\n",
    "                    if val==-100:\n",
    "                        binary_tensors_row.append(torch.full((self.bit_size,), -1))\n",
    "                    else:\n",
    "                        bin_tensor = torch.tensor(token_binary_map[str(val)])\n",
    "                        binary_tensors_row.append(bin_tensor)\n",
    "                binary_tensors.append(torch.stack(binary_tensors_row))\n",
    "            binary_tensors = torch.stack(binary_tensors)\n",
    "            binary_tensors = binary_tensors.to(logits.device)\n",
    "            loss_fct = MaskedLoss()\n",
    "            for j in range(logits.shape[-1]):  # Loop over each classifier\n",
    "                # Compute loss for the j-th node in the final layer\n",
    "                node_loss = loss_fct(shift_logits[:,:, j].float(), binary_tensors[:,:, j].float())\n",
    "                loss += node_loss\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
    "    ):\n",
    "        if past_key_values is not None:\n",
    "            past_length = past_key_values[0][0].shape[2]\n",
    "\n",
    "            # Some generation methods already pass only the last input ID\n",
    "            if input_ids.shape[1] > past_length:\n",
    "                remove_prefix_length = past_length\n",
    "            else:\n",
    "                # Default to old behavior: keep only final ID\n",
    "                remove_prefix_length = input_ids.shape[1] - 1\n",
    "\n",
    "            input_ids = input_ids[:, remove_prefix_length:]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = OPTForCausalLM.from_pretrained(FILE_PATH, return_dict=True, load_in_8bit=False, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_bin_tensor(val, bit_size):\n",
    "    if val==-100:\n",
    "        length = bit_size\n",
    "        bin_str = format(2, '0' + str(length) + 'b')\n",
    "        bin_tensor = torch.tensor([int(bit) for bit in bin_str])\n",
    "    else:\n",
    "        length = bit_size\n",
    "        bin_str = format(val, '0' + str(length) + 'b')\n",
    "        bin_tensor = torch.tensor([int(bit) for bit in bin_str])\n",
    "    return bin_tensor\n",
    "\n",
    "\n",
    "def bin_tensor_to_int(bin_tensor):\n",
    "    \"\"\"Convert a binary tensor to an integer.\"\"\"\n",
    "    bin_str = ''.join(str(bit.item()) for bit in bin_tensor)\n",
    "    return int(bin_str, 2)\n",
    "\n",
    "def convert_vocabulary_to_binary(tokenizer):\n",
    "    \"\"\"Convert all token IDs in the tokenizer's vocabulary to binary with specified bit size.\"\"\"\n",
    "    # Get the vocabulary\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    bit_size = math.ceil(math.log2(tokenizer.vocab_size))\n",
    "    \n",
    "    # Convert each token ID to binary with the specified bit size\n",
    "    binary_vocab = [int_to_bin_tensor(token_id, bit_size) for token_id in vocab.values()]\n",
    "\n",
    "    # Stack the individual binary code tensors into a single tensor of tensors\n",
    "    binary_vocab_tensor = torch.stack(binary_vocab)\n",
    "\n",
    "    return binary_vocab_tensor\n",
    "\n",
    "binary_vocab = convert_vocabulary_to_binary(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear_heads): ModuleList(\n",
      "    (0-49): 50 x Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=128, bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=False)\n",
      "      (3): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = []\n",
    "for value in token_binary_map.values():\n",
    "    if isinstance(value, list):\n",
    "        tensor = torch.tensor(value)\n",
    "        tensor_list.append(tensor)\n",
    "tensor_list = torch.stack(tensor_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids tensor([[    2, 48134, 41241,    35,  4210,  4591,     5,   511,   445,    25,\n",
      "          1528,    50,  3950,     4, 22560, 41327,    35,    20,  2124,   837,\n",
      "            16,     5,  1609,   461,    11,     5,   382,     4, 22560, 19121,\n",
      "            35,  1437]], device='cuda:0')\n",
      "Generated text: ### Instruction: Classify the following statement as true or false. ### Input: The Supreme Court is the highest court in the US. ### Response: \n",
      "The San Francisco-based United States south of the at least, the United States at\n"
     ]
    }
   ],
   "source": [
    "input_text= \"### Instruction: Classify the following statement as true or false. ### Input: The Supreme Court is the highest court in the US. ### Response: \"\n",
    "# Encode the input text to get input_ids\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').cuda()\n",
    "print(\"Input ids\", input_ids)\n",
    "max_length = 50\n",
    "generated_ids = input_ids\n",
    "def bin_tensor_to_int(bin_tensor):\n",
    "    \"\"\"Convert a binary tensor to an integer.\"\"\"\n",
    "    bin_str = ''.join(str(bit.item()) for bit in bin_tensor)\n",
    "    return int(bin_str, 2)\n",
    "\n",
    "for _ in range(max_length - input_ids.size(1)):\n",
    "    # Prepare the model inputs\n",
    "    model_inputs = model.prepare_inputs_for_generation(generated_ids)\n",
    "    \n",
    "    # Get the model outputs\n",
    "    outputs = model(**model_inputs, return_dict=True)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    next_token = model.find_closest_tensor(next_token_logits.to(next_token_logits.device), tensor_list.to(next_token_logits.device))\n",
    "    binary_string = ''.join(str(int(bit)) for bit in next_token[0])\n",
    "    next_token_id = torch.tensor(token_binary_map[binary_string]).to(generated_ids.device)\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
    "    \n",
    "    # Stop generation if end-of-sequence token is generated (optional)\n",
    "    if next_token_id == tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "# # Decode the generated output to text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
